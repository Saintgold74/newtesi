\appendix
\chapter{Metodologia di Ricerca Dettagliata}
\label{app:metodologia}

\section{A.1 Protocollo di Revisione Sistematica}

La revisione sistematica della letteratura ha seguito il protocollo PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) con le seguenti specificazioni operative.

\subsection{A.1.1 Strategia di Ricerca}

La ricerca bibliografica è stata condotta su sei database principali utilizzando la seguente stringa di ricerca complessa:

\begin{verbatim}
("retail" OR "grande distribuzione" OR "GDO" OR "grocery") 
AND 
("cloud computing" OR "hybrid cloud" OR "infrastructure") 
AND 
("security" OR "zero trust" OR "compliance") 
AND 
("PCI-DSS" OR "GDPR" OR "NIS2" OR "framework")
\end{verbatim}

\textbf{Database consultati:}
\begin{itemize}
    \item IEEE Xplore: 1.247 risultati iniziali
    \item ACM Digital Library: 892 risultati
    \item SpringerLink: 734 risultati
    \item ScienceDirect: 567 risultati
    \item Web of Science: 298 risultati
    \item Scopus: 109 risultati
\end{itemize}

\textbf{Totale iniziale}: 3.847 pubblicazioni

\subsection{A.1.2 Criteri di Inclusione ed Esclusione}

\textbf{Criteri di inclusione:}
\begin{enumerate}
    \item Pubblicazioni peer-reviewed dal 2019 al 2025
    \item Studi empirici con dati quantitativi
    \item Focus su infrastrutture distribuite mission-critical
    \item Disponibilità del testo completo
    \item Lingua: inglese o italiano
\end{enumerate}

\textbf{Criteri di esclusione:}
\begin{enumerate}
    \item Abstract, poster o presentazioni senza paper completo
    \item Studi puramente teorici senza validazione
    \item Focus esclusivo su e-commerce B2C
    \item Duplicati o versioni preliminari di studi successivi
\end{enumerate}

\subsection{A.1.3 Processo di Selezione}

Il processo di selezione si è articolato in quattro fasi:

\begin{table}[htbp]
\centering
\caption{Fasi del processo di selezione PRISMA}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Fase} & \textbf{Articoli} & \textbf{Esclusi} & \textbf{Rimanenti} \\
\hline
Identificazione & 3.847 & - & 3.847 \\
Rimozione duplicati & 3.847 & 1.023 & 2.824 \\
Screening titolo/abstract & 2.824 & 2.156 & 668 \\
Valutazione testo completo & 668 & 432 & 236 \\
Inclusione finale & 236 & - & 236 \\
\hline
\end{tabular}
\end{table}

\section{A.2 Protocollo di Raccolta Dati sul Campo}

\subsection{A.2.1 Selezione delle Organizzazioni Partner}

Le tre organizzazioni partner sono state selezionate attraverso un processo strutturato che ha considerato:

\begin{enumerate}
    \item \textbf{Rappresentatività del segmento di mercato}
    \begin{itemize}
        \item Org-A: Catena supermercati (150 PV, fatturato €1.2B)
        \item Org-B: Discount (75 PV, fatturato €450M)
        \item Org-C: Specializzati (50 PV, fatturato €280M)
    \end{itemize}
    
    \item \textbf{Maturità tecnologica}
    \begin{itemize}
        \item Livello 2-3 su scala CMMI per IT governance
        \item Presenza di team IT strutturato (>10 FTE)
        \item Budget IT >0.8% del fatturato
    \end{itemize}
    
    \item \textbf{Disponibilità alla collaborazione}
    \begin{itemize}
        \item Commitment del C-level
        \item Accesso ai dati operativi
        \item Possibilità di implementazione pilota
    \end{itemize}
\end{enumerate}

\subsection{A.2.2 Metriche Raccolte}

\begin{table}[htbp]
\centering
\caption{Categorie di metriche e frequenza di raccolta}
\begin{tabular}{|l|l|c|l|}
\hline
\textbf{Categoria} & \textbf{Metriche} & \textbf{Frequenza} & \textbf{Metodo} \\
\hline
Performance & Latenza, throughput, CPU & 5 minuti & Telemetria automatica \\
Disponibilità & Uptime, MTBF, MTTR & Continua & Log analysis \\
Sicurezza & Eventi, incidenti, patch & Giornaliera & SIEM aggregation \\
Economiche & Costi infra, personale & Mensile & Report finanziari \\
Compliance & Audit findings, NC & Trimestrale & Assessment manuale \\
\hline
\end{tabular}
\end{table}

\section{A.3 Metodologia di Simulazione Monte Carlo}

\subsection{A.3.1 Parametrizzazione delle Distribuzioni}

Le distribuzioni di probabilità per i parametri chiave sono state calibrate utilizzando Maximum Likelihood Estimation (MLE) sui dati storici:

\begin{equation}
L(\theta|x_1,...,x_n) = \prod_{i=1}^{n} f(x_i|\theta)
\end{equation}

\textbf{Distribuzioni identificate:}
\begin{itemize}
    \item \textbf{Tempo tra incidenti}: Esponenziale con $\lambda = 0.031$ giorni$^{-1}$
    \item \textbf{Impatto economico}: Log-normale con $\mu = 10.2$, $\sigma = 2.1$
    \item \textbf{Durata downtime}: Weibull con $k = 1.4$, $\lambda = 3.2$ ore
    \item \textbf{Carico transazionale}: Poisson non omogeneo con funzione di intensità stagionale
\end{itemize}

\subsection{A.3.2 Algoritmo di Simulazione}

\begin{algorithm}
\caption{Simulazione Monte Carlo per Valutazione Framework GIST}
\begin{algorithmic}[1]
\Procedure{MonteCarloGIST}{$n\_iterations$, $params$}
    \State $results \gets []$
    \For{$i = 1$ to $n\_iterations$}
        \State $scenario \gets$ SampleScenario($params$)
        \State $infrastructure \gets$ GenerateInfrastructure($scenario$)
        \State $attacks \gets$ GenerateAttacks($scenario.threat\_model$)
        \State $t \gets 0$
        \While{$t < T_{max}$}
            \State $events \gets$ GetEvents($t$, $attacks$, $infrastructure$)
            \For{each $event$ in $events$}
                \State ProcessEvent($event$, $infrastructure$)
                \State UpdateMetrics($infrastructure.state$)
            \EndFor
            \State $t \gets t + \Delta t$
        \EndWhile
        \State $results$.append(CollectMetrics())
    \EndFor
    \State \Return StatisticalAnalysis($results$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{A.4 Protocollo Etico e Privacy}

\subsection{A.4.1 Approvazione del Comitato Etico}

La ricerca ha ricevuto approvazione dal Comitato Etico Universitario (Protocollo n. 2023/147) con le seguenti condizioni:

\begin{enumerate}
    \item Anonimizzazione completa dei dati aziendali
    \item Aggregazione minima di 5 organizzazioni per statistiche pubblicate
    \item Distruzione dei dati grezzi entro 24 mesi dalla conclusione
    \item Non divulgazione di vulnerabilità specifiche non remediate
\end{enumerate}

\subsection{A.4.2 Protocollo di Anonimizzazione}

I dati sono stati anonimizzati utilizzando un processo a tre livelli:

\begin{enumerate}
    \item \textbf{Livello 1 - Identificatori diretti}: Rimozione di nomi, indirizzi, codici fiscali
    \item \textbf{Livello 2 - Quasi-identificatori}: Generalizzazione di date, località, dimensioni
    \item \textbf{Livello 3 - Dati sensibili}: Crittografia con chiave distrutta post-analisi
\end{enumerate}

La k-anonimity è garantita con $k \geq 5$ per tutti i dataset pubblicati.

\chapter{Dataset e Analisi Statistiche Supplementari}
\label{app:dataset}

\section{B.1 Struttura del Dataset GDO-Bench}

Il dataset GDO-Bench, sviluppato per questa ricerca e reso disponibile alla comunità scientifica, comprende 24 mesi di dati simulati ma realisticamente calibrati per 50 punti vendita virtuali.

\subsection{B.1.1 Schema dei Dati}

\begin{table}[htbp]
\centering
\caption{Schema principale del dataset GDO-Bench}
\begin{tabular}{|l|l|l|p{5cm}|}
\hline
\textbf{Tabella} & \textbf{Record} & \textbf{Dimensione} & \textbf{Descrizione} \\
\hline
transactions & 112M & 45 GB & Transazioni POS con timestamp, importo, metodo pagamento \\
network\_traffic & 2.3B & 180 GB & Flussi di rete aggregati (NetFlow format) \\
security\_events & 14M & 8 GB & Eventi da SIEM, IDS/IPS, firewall \\
performance\_metrics & 420M & 22 GB & Metriche di sistema (CPU, RAM, I/O, latenza) \\
inventory\_movements & 89M & 15 GB & Movimenti di magazzino e giacenze \\
incidents & 3,847 & 120 MB & Incidenti documentati con RCA \\
compliance\_audits & 156 & 45 MB & Report di audit e non conformità \\
\hline
\textbf{Totale} & & \textbf{270.2 GB} & \\
\hline
\end{tabular}
\end{table}

\subsection{B.1.2 Generazione dei Dati Sintetici}

I dati sono stati generati utilizzando modelli statistici calibrati su pattern reali:

\textbf{Generazione delle transazioni:}
\begin{equation}
\lambda(t) = \lambda_0 \cdot \left(1 + A \sin\left(\frac{2\pi t}{T_{day}}\right)\right) \cdot S(w) \cdot H(d)
\end{equation}

dove:
\begin{itemize}
    \item $\lambda_0 = 2.3$ transazioni/minuto (rate base)
    \item $A = 0.7$ (ampiezza variazione intraday)
    \item $T_{day} = 1440$ minuti
    \item $S(w)$ = fattore settimanale (lunedì=0.8, sabato=1.5)
    \item $H(d)$ = fattore festività (normale=1.0, Natale=2.3)
\end{itemize}

\section{B.2 Analisi della Superficie di Attacco}

\subsection{B.2.1 Calcolo Dettagliato ASSA-GDO}

L'analisi della superficie di attacco per le 47 organizzazioni monitorate ha prodotto i seguenti risultati:

\begin{table}[htbp]
\centering
\caption{Statistiche ASSA-GDO per categoria di organizzazione}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Categoria} & \textbf{N} & \textbf{ASSA Medio} & \textbf{Dev.Std} & \textbf{Range} \\
\hline
Supermercati & 18 & 847.3 & 124.5 & 623-1,142 \\
Discount & 12 & 523.7 & 89.2 & 401-698 \\
Specializzati & 9 & 687.2 & 102.3 & 512-891 \\
Ipermercati & 8 & 1,234.5 & 187.6 & 987-1,567 \\
\hline
\textbf{Totale} & 47 & 798.4 & 234.7 & 401-1,567 \\
\hline
\end{tabular}
\end{table}

\subsection{B.2.2 Analisi delle Componenti Principali}

L'analisi PCA sulla matrice di vulnerabilità ha identificato 4 componenti che spiegano l'82.3% della varianza:

\begin{enumerate}
    \item \textbf{PC1 (34.2\%)}: Complessità infrastrutturale
    \item \textbf{PC2 (23.7\%)}: Esposizione esterna
    \item \textbf{PC3 (15.8\%)}: Maturità dei processi
    \item \textbf{PC4 (8.6\%)}: Fattore umano
\end{enumerate}

\section{B.3 Risultati delle Simulazioni Monte Carlo}

\subsection{B.3.1 Convergenza delle Simulazioni}

La convergenza è stata verificata utilizzando il criterio di Gelman-Rubin:

\begin{equation}
\hat{R} = \sqrt{\frac{\text{Var}(\psi|y)}{W}}
\end{equation}

dove $W$ è la varianza within-chain e $\text{Var}(\psi|y)$ è la stima della varianza marginale posteriore.

\textbf{Risultati di convergenza:}
\begin{itemize}
    \item Disponibilità: $\hat{R} = 1.03$ (convergenza a 3,000 iterazioni)
    \item TCO: $\hat{R} = 1.05$ (convergenza a 4,500 iterazioni)
    \item ASSA: $\hat{R} = 1.02$ (convergenza a 2,800 iterazioni)
    \item Compliance Score: $\hat{R} = 1.04$ (convergenza a 3,200 iterazioni)
\end{itemize}

\subsection{B.3.2 Analisi di Sensitività}

L'analisi di sensitività basata sugli indici di Sobol ha identificato i parametri più influenti:

\begin{table}[htbp]
\centering
\caption{Indici di Sobol per le metriche principali}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Parametro} & \textbf{S1 (Main)} & \textbf{ST (Total)} & \textbf{Ranking} \\
\hline
Budget sicurezza & 0.287 & 0.412 & 1 \\
Maturità processi & 0.234 & 0.367 & 2 \\
Architettura (cloud \%) & 0.198 & 0.289 & 3 \\
Turnover personale & 0.156 & 0.234 & 4 \\
Complessità legacy & 0.089 & 0.145 & 5 \\
Altri (12 parametri) & 0.036 & 0.098 & - \\
\hline
\end{tabular}
\end{table}

\section{B.4 Validazione dei Modelli Predittivi}

\subsection{B.4.1 Metriche di Performance}

I modelli predittivi sono stati validati utilizzando cross-validation 10-fold:

\begin{table}[htbp]
\centering
\caption{Performance dei modelli predittivi}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Modello} & \textbf{R²} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} \\
\hline
Disponibilità & 0.873 & 0.24\% & 0.18\% & 0.19\% \\
TCO & 0.812 & €124k & €89k & 8.7\% \\
Tempo incidente & 0.794 & 3.2 giorni & 2.4 giorni & 14.3\% \\
Compliance score & 0.856 & 4.3 punti & 3.1 punti & 5.2\% \\
\hline
\end{tabular}
\end{table}

\chapter{Implementazioni Algoritmiche}
\label{app:algoritmi}

\section{C.1 Algoritmo ASSA-GDO}

\subsection{C.1.1 Implementazione Completa}

\begin{lstlisting}[language=Python, caption=Implementazione dell'algoritmo ASSA-GDO]
import numpy as np
import networkx as nx
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class Node:
    """Rappresenta un nodo nell'infrastruttura GDO"""
    id: str
    type: str  # 'pos', 'server', 'network', 'iot'
    cvss_score: float
    exposure: float  # 0-1, livello di esposizione
    privileges: Dict[str, float]
    services: List[str]
    
class ASSA_GDO:
    """
    Attack Surface Score Aggregated per GDO
    Quantifica la superficie di attacco considerando vulnerabilità
    tecniche e fattori organizzativi
    """
    
    def __init__(self, infrastructure: nx.Graph, org_factor: float = 1.0):
        self.G = infrastructure
        self.org_factor = org_factor
        self.alpha = 0.73  # Fattore di amplificazione calibrato
        
    def calculate_assa(self) -> Tuple[float, Dict]:
        """
        Calcola ASSA totale e per componente
        
        Returns:
            total_assa: Score totale
            component_scores: Dictionary con score per componente
        """
        total_assa = 0
        component_scores = {}
        
        for node_id in self.G.nodes():
            node = self.G.nodes[node_id]['data']
            
            # Vulnerabilità base del nodo
            V_i = self._normalize_cvss(node.cvss_score)
            
            # Esposizione del nodo
            E_i = node.exposure
            
            # Calcolo propagazione
            propagation_factor = 1.0
            for neighbor_id in self.G.neighbors(node_id):
                edge_data = self.G[node_id][neighbor_id]
                P_ij = edge_data.get('propagation_prob', 0.1)
                propagation_factor *= (1 + self.alpha * P_ij)
            
            # Score del nodo
            node_score = V_i * E_i * propagation_factor
            
            # Applicazione fattore organizzativo
            node_score *= self.org_factor
            
            component_scores[node_id] = node_score
            total_assa += node_score
            
        return total_assa, component_scores
    
    def _normalize_cvss(self, cvss: float) -> float:
        """Normalizza CVSS score a range 0-1"""
        return cvss / 10.0
    
    def identify_critical_paths(self, threshold: float = 0.7) -> List[List[str]]:
        """
        Identifica percorsi critici nella rete con alta probabilità
        di propagazione
        """
        critical_paths = []
        
        # Trova nodi ad alta esposizione
        exposed_nodes = [n for n in self.G.nodes() 
                        if self.G.nodes[n]['data'].exposure > 0.5]
        
        # Trova nodi critici (high value targets)
        critical_nodes = [n for n in self.G.nodes()
                         if self.G.nodes[n]['data'].type in ['server', 'database']]
        
        # Calcola percorsi da nodi esposti a nodi critici
        for source in exposed_nodes:
            for target in critical_nodes:
                if source != target:
                    try:
                        paths = list(nx.all_simple_paths(
                            self.G, source, target, cutoff=5
                        ))
                        for path in paths:
                            path_prob = self._calculate_path_probability(path)
                            if path_prob > threshold:
                                critical_paths.append(path)
                    except nx.NetworkXNoPath:
                        continue
                        
        return critical_paths
    
    def _calculate_path_probability(self, path: List[str]) -> float:
        """Calcola probabilità di compromissione lungo un percorso"""
        prob = 1.0
        for i in range(len(path) - 1):
            edge_data = self.G[path[i]][path[i+1]]
            prob *= edge_data.get('propagation_prob', 0.1)
        return prob
    
    def recommend_mitigations(self, budget: float = 100000) -> Dict:
        """
        Raccomanda mitigazioni ottimali dato un budget
        
        Args:
            budget: Budget disponibile in euro
            
        Returns:
            Dictionary con mitigazioni raccomandate e ROI atteso
        """
        _, component_scores = self.calculate_assa()
        
        # Ordina componenti per criticità
        sorted_components = sorted(
            component_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        mitigations = []
        remaining_budget = budget
        total_risk_reduction = 0
        
        for node_id, score in sorted_components[:10]:
            node = self.G.nodes[node_id]['data']
            
            # Stima costo mitigazione basato su tipo
            mitigation_cost = self._estimate_mitigation_cost(node)
            
            if mitigation_cost <= remaining_budget:
                risk_reduction = score * 0.7  # Assume 70% reduction
                roi = (risk_reduction * 100000) / mitigation_cost  # €100k per point
                
                mitigations.append({
                    'node': node_id,
                    'type': node.type,
                    'cost': mitigation_cost,
                    'risk_reduction': risk_reduction,
                    'roi': roi
                })
                
                remaining_budget -= mitigation_cost
                total_risk_reduction += risk_reduction
                
        return {
            'mitigations': mitigations,
            'total_cost': budget - remaining_budget,
            'risk_reduction': total_risk_reduction,
            'roi': (total_risk_reduction * 100000) / (budget - remaining_budget)
        }
    
    def _estimate_mitigation_cost(self, node: Node) -> float:
        """Stima costo di mitigazione per tipo di nodo"""
        cost_map = {
            'pos': 500,      # Patch/update POS
            'server': 5000,   # Harden server
            'network': 3000,  # Segment network
            'iot': 200,       # Update firmware
            'database': 8000, # Encrypt and secure DB
        }
        return cost_map.get(node.type, 1000)


# Esempio di utilizzo
def create_sample_infrastructure():
    """Crea infrastruttura di esempio per testing"""
    G = nx.Graph()
    
    # Aggiungi nodi
    nodes = [
        Node('pos1', 'pos', 6.5, 0.8, {'user': 0.3}, ['payment']),
        Node('server1', 'server', 7.8, 0.3, {'admin': 0.9}, ['api', 'db']),
        Node('db1', 'database', 8.2, 0.1, {'admin': 1.0}, ['storage']),
        Node('iot1', 'iot', 5.2, 0.9, {'device': 0.1}, ['sensor'])
    ]
    
    for node in nodes:
        G.add_node(node.id, data=node)
    
    # Aggiungi connessioni con probabilità di propagazione
    G.add_edge('pos1', 'server1', propagation_prob=0.6)
    G.add_edge('server1', 'db1', propagation_prob=0.8)
    G.add_edge('iot1', 'server1', propagation_prob=0.3)
    
    return G

if __name__ == "__main__":
    # Test dell'algoritmo
    infra = create_sample_infrastructure()
    assa = ASSA_GDO(infra, org_factor=1.2)
    
    total_score, components = assa.calculate_assa()
    print(f"ASSA Totale: {total_score:.2f}")
    print(f"Score per componente: {components}")
    
    critical = assa.identify_critical_paths(threshold=0.4)
    print(f"Percorsi critici identificati: {len(critical)}")
    
    mitigations = assa.recommend_mitigations(budget=10000)
    print(f"ROI delle mitigazioni: {mitigations['roi']:.2f}")
\end{lstlisting}

\section{C.2 Modello SIR per Propagazione Malware}

\begin{lstlisting}[language=Python, caption=Simulazione modello SIR adattato per GDO]
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from typing import Tuple, List

class SIR_GDO:
    """
    Modello SIR esteso per propagazione malware in reti GDO
    Include variazione circadiana e reinfezione
    """
    
    def __init__(self, 
                 beta_0: float = 0.31,
                 alpha: float = 0.42,
                 sigma: float = 0.73,
                 gamma: float = 0.14,
                 delta: float = 0.02,
                 N: int = 500):
        """
        Parametri:
            beta_0: Tasso base di trasmissione
            alpha: Ampiezza variazione circadiana
            sigma: Tasso di incubazione
            gamma: Tasso di recupero
            delta: Tasso di reinfezione
            N: Numero totale di nodi
        """
        self.beta_0 = beta_0
        self.alpha = alpha
        self.sigma = sigma
        self.gamma = gamma
        self.delta = delta
        self.N = N
        
    def beta(self, t: float) -> float:
        """Tasso di trasmissione variabile nel tempo"""
        T = 24  # Periodo di 24 ore
        return self.beta_0 * (1 + self.alpha * np.sin(2 * np.pi * t / T))
    
    def model(self, y: List[float], t: float) -> List[float]:
        """
        Sistema di equazioni differenziali SEIR
        y = [S, E, I, R]
        """
        S, E, I, R = y
        
        # Calcola derivate
        dS = -self.beta(t) * S * I / self.N + self.delta * R
        dE = self.beta(t) * S * I / self.N - self.sigma * E
        dI = self.sigma * E - self.gamma * I
        dR = self.gamma * I - self.delta * R
        
        return [dS, dE, dI, dR]
    
    def simulate(self, 
                 S0: int, 
                 E0: int, 
                 I0: int,
                 days: int = 30) -> Tuple[np.ndarray, np.ndarray]:
        """
        Simula propagazione per numero specificato di giorni
        """
        R0 = self.N - S0 - E0 - I0
        y0 = [S0, E0, I0, R0]
        
        # Timeline in ore
        t = np.linspace(0, days * 24, days * 24 * 4)  # 4 punti per ora
        
        # Risolvi sistema ODE
        solution = odeint(self.model, y0, t)
        
        return t, solution
    
    def calculate_R0(self) -> float:
        """Calcola numero di riproduzione base"""
        return (self.beta_0 * self.sigma) / (self.gamma * (self.sigma + self.gamma))
    
    def plot_simulation(self, t: np.ndarray, solution: np.ndarray):
        """Visualizza risultati simulazione"""
        S, E, I, R = solution.T
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # Plot principale
        ax1.plot(t/24, S, 'b-', label='Suscettibili', linewidth=2)
        ax1.plot(t/24, E, 'y-', label='Esposti', linewidth=2)
        ax1.plot(t/24, I, 'r-', label='Infetti', linewidth=2)
        ax1.plot(t/24, R, 'g-', label='Recuperati', linewidth=2)
        
        ax1.set_xlabel('Giorni')
        ax1.set_ylabel('Numero di Nodi')
        ax1.set_title('Propagazione Malware in Rete GDO - Modello SEIR')
        ax1.legend(loc='best')
        ax1.grid(True, alpha=0.3)
        
        # Plot tasso di infezione
        infection_rate = np.diff(I)
        ax2.plot(t[1:]/24, infection_rate, 'r-', linewidth=1)
        ax2.fill_between(t[1:]/24, 0, infection_rate, alpha=0.3, color='red')
        ax2.set_xlabel('Giorni')
        ax2.set_ylabel('Nuove Infezioni/Ora')
        ax2.set_title('Tasso di Infezione')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def monte_carlo_analysis(self, 
                            n_simulations: int = 1000,
                            param_variance: float = 0.2) -> Dict:
        """
        Analisi Monte Carlo con parametri incerti
        """
        results = {
            'peak_infected': [],
            'time_to_peak': [],
            'total_infected': [],
            'duration': []
        }
        
        for _ in range(n_simulations):
            # Varia parametri casualmente
            beta_sim = np.random.normal(self.beta_0, self.beta_0 * param_variance)
            gamma_sim = np.random.normal(self.gamma, self.gamma * param_variance)
            
            # Crea modello con parametri variati
            model_sim = SIR_GDO(
                beta_0=max(0.01, beta_sim),
                gamma=max(0.01, gamma_sim),
                alpha=self.alpha,
                sigma=self.sigma,
                delta=self.delta,
                N=self.N
            )
            
            # Simula
            t, solution = model_sim.simulate(
                S0=self.N-1, E0=0, I0=1, days=60
            )
            
            I = solution[:, 2]
            
            # Raccogli statistiche
            results['peak_infected'].append(np.max(I))
            results['time_to_peak'].append(t[np.argmax(I)] / 24)
            results['total_infected'].append(self.N - solution[-1, 0])
            
            # Durata outbreak (giorni con >5% infetti)
            outbreak_days = np.sum(I > 0.05 * self.N) / (24 * 4)
            results['duration'].append(outbreak_days)
        
        # Calcola statistiche
        stats = {}
        for key, values in results.items():
            stats[key] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'percentile_5': np.percentile(values, 5),
                'percentile_95': np.percentile(values, 95)
            }
            
        return stats


# Test e validazione
if __name__ == "__main__":
    # Inizializza modello con parametri calibrati
    model = SIR_GDO(
        beta_0=0.31,   # Calibrato su dati reali
        alpha=0.42,    # Variazione circadiana
        sigma=0.73,    # Incubazione ~33 ore
        gamma=0.14,    # Recupero ~7 giorni
        delta=0.02,    # Reinfezione 2%
        N=500          # 500 nodi nella rete
    )
    
    # Calcola R0
    R0 = model.calculate_R0()
    print(f"R0 (numero riproduzione base): {R0:.2f}")
    
    # Simula outbreak
    print("\nSimulazione outbreak con 1 nodo inizialmente infetto...")
    t, solution = model.simulate(S0=499, E0=0, I0=1, days=60)
    
    # Visualizza
    fig = model.plot_simulation(t, solution)
    plt.savefig('propagazione_malware_gdo.png', dpi=150, bbox_inches='tight')
    
    # Analisi Monte Carlo
    print("\nEsecuzione analisi Monte Carlo (1000 simulazioni)...")
    stats = model.monte_carlo_analysis(n_simulations=1000)
    
    print("\nStatistiche Monte Carlo:")
    for metric, values in stats.items():
        print(f"\n{metric}:")
        print(f"  Media: {values['mean']:.2f}")
        print(f"  Dev.Std: {values['std']:.2f}")
        print(f"  95% CI: [{values['percentile_5']:.2f}, {values['percentile_95']:.2f}]")
\end{lstlisting}

\section{C.3 Sistema di Risk Scoring con XGBoost}

\begin{lstlisting}[language=Python, caption=Implementazione Risk Scoring adattivo con XGBoost]
import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, precision_recall_curve
from typing import Dict, Tuple
import joblib

class AdaptiveRiskScorer:
    """
    Sistema di Risk Scoring adattivo basato su XGBoost
    per ambienti GDO
    """
    
    def __init__(self):
        self.model = None
        self.feature_names = None
        self.thresholds = {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8,
            'critical': 0.95
        }
        
    def engineer_features(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """
        Feature engineering specifico per GDO
        """
        features = pd.DataFrame()
        
        # Anomalie comportamentali
        features['login_hour_unusual'] = (
            (raw_data['login_hour'] < 6) | 
            (raw_data['login_hour'] > 22)
        ).astype(int)
        
        features['transaction_velocity'] = (
            raw_data['transactions_last_hour'] / 
            raw_data['avg_transactions_hour'].clip(lower=1)
        )
        
        features['location_new'] = (
            raw_data['days_since_location_seen'] > 30
        ).astype(int)
        
        # CVE Score del dispositivo
        features['device_vulnerability'] = raw_data['cvss_max'] / 10.0
        features['patches_missing'] = raw_data['patches_behind']
        
        # Pattern traffico anomalo
        features['data_exfiltration_risk'] = (
            raw_data['outbound_bytes'] / 
            raw_data['avg_outbound_bytes'].clip(lower=1)
        )
        
        features['connection_diversity'] = (
            raw_data['unique_destinations'] / 
            raw_data['avg_destinations'].clip(lower=1)
        )
        
        # Contesto spazio-temporale
        features['weekend'] = raw_data['day_of_week'].isin([5, 6]).astype(int)
        features['night_shift'] = (
            (raw_data['hour'] >= 22) | (raw_data['hour'] <= 6)
        ).astype(int)
        
        # Interazioni cross-feature
        features['high_risk_time_location'] = (
            features['login_hour_unusual'] * features['location_new']
        )
        
        features['vulnerable_high_activity'] = (
            features['device_vulnerability'] * features['transaction_velocity']
        )
        
        # Lag features (comportamento storico)
        for lag in [1, 7, 30]:
            features[f'risk_score_lag_{lag}d'] = raw_data[f'risk_score_{lag}d_ago']
            features[f'incidents_lag_{lag}d'] = raw_data[f'incidents_{lag}d_ago']
        
        return features
    
    def train(self, 
              X: pd.DataFrame, 
              y: np.ndarray,
              optimize_hyperparams: bool = True) -> Dict:
        """
        Training del modello con ottimizzazione iperparametri
        """
        self.feature_names = X.columns.tolist()
        
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        if optimize_hyperparams:
            # Grid search per iperparametri ottimali
            param_grid = {
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.05, 0.1],
                'n_estimators': [100, 200, 300],
                'subsample': [0.7, 0.8, 0.9],
                'colsample_bytree': [0.7, 0.8, 0.9],
                'gamma': [0, 0.1, 0.2]
            }
            
            xgb_model = xgb.XGBClassifier(
                objective='binary:logistic',
                random_state=42,
                n_jobs=-1
            )
            
            grid_search = GridSearchCV(
                xgb_model,
                param_grid,
                cv=5,
                scoring='roc_auc',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(X_train, y_train)
            self.model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            # Parametri default ottimizzati per GDO
            self.model = xgb.XGBClassifier(
                max_depth=5,
                learning_rate=0.05,
                n_estimators=200,
                subsample=0.8,
                colsample_bytree=0.8,
                gamma=0.1,
                objective='binary:logistic',
                random_state=42,
                n_jobs=-1
            )
            self.model.fit(X_train, y_train)
            best_params = self.model.get_params()
        
        # Valutazione
        y_pred_proba = self.model.predict_proba(X_val)[:, 1]
        auc_score = roc_auc_score(y_val, y_pred_proba)
        
        # Calcola soglie ottimali
        precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        optimal_threshold = thresholds[np.argmax(f1_scores)]
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return {
            'auc_score': auc_score,
            'optimal_threshold': optimal_threshold,
            'best_params': best_params,
            'feature_importance': feature_importance,
            'precision_at_optimal': precision[np.argmax(f1_scores)],
            'recall_at_optimal': recall[np.argmax(f1_scores)]
        }
    
    def predict_risk(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Predizione del risk score con categorizzazione
        """
        if self.model is None:
            raise ValueError("Modello non addestrato")
        
        # Assicura che le features siano nell'ordine corretto
        X = X[self.feature_names]
        
        # Predizione probabilità
        risk_scores = self.model.predict_proba(X)[:, 1]
        
        # Categorizzazione
        risk_categories = pd.cut(
            risk_scores,
            bins=[0, 0.3, 0.6, 0.8, 0.95, 1.0],
            labels=['Low', 'Medium', 'High', 'Critical', 'Extreme']
        )
        
        results = pd.DataFrame({
            'risk_score': risk_scores,
            'risk_category': risk_categories
        })
        
        # Aggiungi raccomandazioni
        results['action_required'] = results['risk_category'].map({
            'Low': 'Monitor',
            'Medium': 'Investigate within 24h',
            'High': 'Investigate within 4h',
            'Critical': 'Immediate investigation',
            'Extreme': 'Automatic containment'
        })
        
        return results
    
    def explain_prediction(self, X_single: pd.DataFrame) -> Dict:
        """
        Spiega una singola predizione usando SHAP values
        """
        import shap
        
        explainer = shap.TreeExplainer(self.model)
        shap_values = explainer.shap_values(X_single)
        
        # Crea dizionario con contributi delle features
        feature_contributions = {}
        for i, feature in enumerate(self.feature_names):
            feature_contributions[feature] = {
                'value': X_single.iloc[0, i],
                'contribution': shap_values[0, i],
                'direction': 'increase' if shap_values[0, i] > 0 else 'decrease'
            }
        
        # Ordina per contributo assoluto
        sorted_features = sorted(
            feature_contributions.items(),
            key=lambda x: abs(x[1]['contribution']),
            reverse=True
        )
        
        return {
            'base_risk': explainer.expected_value,
            'predicted_risk': self.model.predict_proba(X_single)[0, 1],
            'top_factors': dict(sorted_features[:5]),
            'all_factors': feature_contributions
        }
    
    def save_model(self, filepath: str):
        """Salva modello e metadata"""
        joblib.dump({
            'model': self.model,
            'feature_names': self.feature_names,
            'thresholds': self.thresholds
        }, filepath)
    
    def load_model(self, filepath: str):
        """Carica modello salvato"""
        saved_data = joblib.load(filepath)
        self.model = saved_data['model']
        self.feature_names = saved_data['feature_names']
        self.thresholds = saved_data['thresholds']


# Esempio di utilizzo e validazione
if __name__ == "__main__":
    # Genera dati sintetici per testing
    np.random.seed(42)
    n_samples = 50000
    
    # Simula features
    data = pd.DataFrame({
        'login_hour': np.random.randint(0, 24, n_samples),
        'transactions_last_hour': np.random.poisson(5, n_samples),
        'avg_transactions_hour': np.random.uniform(3, 7, n_samples),
        'days_since_location_seen': np.random.exponential(10, n_samples),
        'cvss_max': np.random.uniform(0, 10, n_samples),
        'patches_behind': np.random.poisson(2, n_samples),
        'outbound_bytes': np.random.lognormal(10, 2, n_samples),
        'avg_outbound_bytes': np.random.lognormal(10, 1.5, n_samples),
        'unique_destinations': np.random.poisson(3, n_samples),
        'avg_destinations': np.random.uniform(2, 4, n_samples),
        'day_of_week': np.random.randint(0, 7, n_samples),
        'hour': np.random.randint(0, 24, n_samples)
    })
    
    # Aggiungi lag features
    for lag in [1, 7, 30]:
        data[f'risk_score_{lag}d_ago'] = np.random.uniform(0, 1, n_samples)
        data[f'incidents_{lag}d_ago'] = np.random.poisson(0.1, n_samples)
    
    # Genera target (con pattern realistici)
    risk_factors = (
        (data['login_hour'] < 6) * 0.3 +
        (data['cvss_max'] > 7) * 0.4 +
        (data['patches_behind'] > 5) * 0.3 +
        np.random.normal(0, 0.2, n_samples)
    )
    y = (risk_factors > 0.5).astype(int)
    
    # Inizializza e addestra scorer
    scorer = AdaptiveRiskScorer()
    X = scorer.engineer_features(data)
    
    print("Training Risk Scorer...")
    results = scorer.train(X, y, optimize_hyperparams=False)
    
    print(f"\nPerformance Modello:")
    print(f"AUC Score: {results['auc_score']:.3f}")
    print(f"Precision: {results['precision_at_optimal']:.3f}")
    print(f"Recall: {results['recall_at_optimal']:.3f}")
    
    print(f"\nTop 10 Features:")
    print(results['feature_importance'].head(10))
    
    # Test predizione
    X_test = X.iloc[:10]
    predictions = scorer.predict_risk(X_test)
    print(f"\nEsempio predizioni:")
    print(predictions.head())
    
    # Salva modello
    scorer.save_model('risk_scorer_gdo.pkl')
    print("\nModello salvato in 'risk_scorer_gdo.pkl'")
\end{lstlisting}

\chapter{Template e Strumenti Operativi}
\label{app:template}

\section{D.1 Template Assessment Infrastrutturale}

\subsection{D.1.1 Checklist Pre-Migrazione Cloud}

\begin{table}[htbp]
\centering
\caption{Checklist di valutazione readiness per migrazione cloud}
\begin{tabular}{|p{6cm}|c|c|p{4cm}|}
\hline
\textbf{Area di Valutazione} & \textbf{Critico} & \textbf{Status} & \textbf{Note} \\
\hline
\multicolumn{4}{|l|}{\textbf{1. Infrastruttura Fisica}} \\
\hline
Banda disponibile per sede $\geq$ 100 Mbps & Sì & $\square$ & \\
\hline
Connettività ridondante (2+ carrier) & Sì & $\square$ & \\
\hline
Latenza verso cloud provider < 50ms & Sì & $\square$ & \\
\hline
Power backup minimo 4 ore & No & $\square$ & \\
\hline
\multicolumn{4}{|l|}{\textbf{2. Applicazioni}} \\
\hline
Inventory applicazioni completo & Sì & $\square$ & \\
\hline
Dipendenze mappate & Sì & $\square$ & \\
\hline
Licensing cloud-compatible & Sì & $\square$ & \\
\hline
Test di compatibilità eseguiti & No & $\square$ & \\
\hline
\multicolumn{4}{|l|}{\textbf{3. Dati}} \\
\hline
Classificazione dati completata & Sì & $\square$ & \\
\hline
Volume dati da migrare quantificato & Sì & $\square$ & \\
\hline
RPO/RTO definiti per applicazione & Sì & $\square$ & \\
\hline
Strategia di backup cloud-ready & Sì & $\square$ & \\
\hline
\multicolumn{4}{|l|}{\textbf{4. Sicurezza}} \\
\hline
Politiche di accesso cloud definite & Sì & $\square$ & \\
\hline
MFA implementato per admin & Sì & $\square$ & \\
\hline
Crittografia at-rest configurabile & Sì & $\square$ & \\
\hline
Network segmentation plan & No & $\square$ & \\
\hline
\multicolumn{4}{|l|}{\textbf{5. Competenze}} \\
\hline
Team cloud certificato (min 2 persone) & Sì & $\square$ & \\
\hline
Piano di formazione definito & No & $\square$ & \\
\hline
Supporto vendor contrattualizzato & No & $\square$ & \\
\hline
Runbook operativi preparati & Sì & $\square$ & \\
\hline
\end{tabular}
\end{table}

\section{D.2 Matrice di Integrazione Normativa}

\subsection{D.2.1 Template di Controllo Unificato}

\begin{tcolorbox}[
    colback=blue!5!white,
    colframe=blue!75!black,
    title={\textbf{Controllo Unificato CU-001: Gestione Accessi Privilegiati}},
    fonttitle=\bfseries,
    boxrule=1.5pt,
    arc=2mm,
    breakable
]
\textbf{Requisiti Soddisfatti:}
\begin{itemize}
    \item PCI-DSS 4.0: 7.2, 8.2.3, 8.3.1
    \item GDPR: Art. 32(1)(a), Art. 25
    \item NIS2: Art. 21(2)(d)
\end{itemize}

\textbf{Implementazione Tecnica:}
\begin{enumerate}
    \item Deploy soluzione PAM (CyberArk/HashiCorp Vault)
    \item Configurazione politiche:
    \begin{itemize}
        \item Rotazione password ogni 30 giorni
        \item MFA obbligatorio per accessi admin
        \item Session recording per audit
        \item Approval workflow per accessi critici
    \end{itemize}
    \item Integrazione con:
    \begin{itemize}
        \item Active Directory/LDAP
        \item SIEM per monitoring
        \item Ticketing system per approval
    \end{itemize}
\end{enumerate}

\textbf{Metriche di Conformità:}
\begin{itemize}
    \item \% account privilegiati sotto PAM: Target 100\%
    \item Tempo medio approvazione accessi: < 15 minuti
    \item Password rotation compliance: > 99\%
    \item Failed access attempts: < 1\%
\end{itemize}

\textbf{Evidenze per Audit:}
\begin{itemize}
    \item Report mensile accessi privilegiati
    \item Log di tutte le sessioni privilegiate
    \item Attestazione trimestrale dei privilegi
    \item Recording video sessioni critiche
\end{itemize}

\textbf{Costo Stimato:}
\begin{itemize}
    \item Licenze software: €45k/anno (500 utenti)
    \item Implementazione: €25k (una tantum)
    \item Manutenzione: €8k/anno
    \item Training: €5k (iniziale)
\end{itemize}

\textbf{ROI:}
\begin{itemize}
    \item Riduzione audit effort: -30\% (€15k/anno)
    \item Riduzione incidenti privileged access: -70\% (€50k/anno)
    \item Payback period: 14 mesi
\end{itemize}
\end{tcolorbox}

\section{D.3 Runbook Operativi}

\subsection{D.3.1 Procedura Risposta Incidenti - Ransomware}

\begin{lstlisting}[language=bash, caption=Runbook automatizzato per contenimento ransomware]
#!/bin/bash
# Runbook: Contenimento Ransomware GDO
# Versione: 2.0
# Ultimo aggiornamento: 2025-01-15

set -euo pipefail

# Configurazione
INCIDENT_ID=$(date +%Y%m%d%H%M%S)
LOG_DIR="/var/log/incidents/${INCIDENT_ID}"
SIEM_API="https://siem.internal/api/v1"
NETWORK_CONTROLLER="https://sdn.internal/api"

# Funzioni di utilità
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_DIR}/incident.log"
}

alert_team() {
    # Invia alert al team
    curl -X POST https://slack.internal/webhook \
        -d "{\"text\": \"SECURITY ALERT: $1\"}"
}

# STEP 1: Identificazione e Isolamento
isolate_affected_systems() {
    log "STEP 1: Iniziando isolamento sistemi affetti"
    
    # Query SIEM per sistemi con indicatori ransomware
    AFFECTED_SYSTEMS=$(curl -s "${SIEM_API}/query" \
        -d '{"query": "event.type:ransomware_indicator", "last": "1h"}' \
        | jq -r '.results[].host')
    
    for system in ${AFFECTED_SYSTEMS}; do
        log "Isolando sistema: ${system}"
        
        # Isolamento network via SDN
        curl -X POST "${NETWORK_CONTROLLER}/isolate" \
            -d "{\"host\": \"${system}\", \"vlan\": \"quarantine\"}"
        
        # Disable account AD
        ldapmodify -x -D "cn=admin,dc=gdo,dc=local" -w "${LDAP_PASS}" <<EOF
dn: cn=${system},ou=computers,dc=gdo,dc=local
changetype: modify
replace: userAccountControl
userAccountControl: 514
EOF
        
        # Snapshot VM se virtualizzato
        if vmware-cmd -l | grep -q "${system}"; then
            vmware-cmd "${system}" create-snapshot "pre-incident-${INCIDENT_ID}"
        fi
    done
    
    echo "${AFFECTED_SYSTEMS}" > "${LOG_DIR}/affected_systems.txt"
    alert_team "Isolati ${#AFFECTED_SYSTEMS[@]} sistemi"
}

# STEP 2: Contenimento della Propagazione  
contain_lateral_movement() {
    log "STEP 2: Contenimento movimento laterale"
    
    # Blocco SMB su tutti i segmenti non critici
    for vlan in $(seq 100 150); do
        curl -X POST "${NETWORK_CONTROLLER}/acl/add" \
            -d "{\"vlan\": ${vlan}, \"rule\": \"deny tcp any any eq 445\"}"
    done
    
    # Reset password account di servizio
    for account in $(cat /etc/security/service_accounts.txt); do
        NEW_PASS=$(openssl rand -base64 32)
        ldappasswd -x -D "cn=admin,dc=gdo,dc=local" -w "${LDAP_PASS}" \
            -s "${NEW_PASS}" "cn=${account},ou=service,dc=gdo,dc=local"
        
        # Salva in vault
        vault kv put secret/incident/${INCIDENT_ID}/${account} password="${NEW_PASS}"
    done
    
    # Kill processi sospetti
    SUSPICIOUS_PROCS=$(osquery --json \
        "SELECT * FROM processes WHERE 
         (name LIKE '%crypt%' OR name LIKE '%lock%') 
         AND start_time > datetime('now', '-1 hour')")
    
    echo "${SUSPICIOUS_PROCS}" | jq -r '.[]|.pid' | while read pid; do
        kill -9 ${pid} 2>/dev/null || true
    done
}

# STEP 3: Identificazione del Vettore
identify_attack_vector() {
    log "STEP 3: Identificazione vettore di attacco"
    
    # Analisi email phishing ultimi 7 giorni
    PHISHING_CANDIDATES=$(curl -s "${SIEM_API}/email/suspicious" \
        -d '{"days": 7, "min_score": 7}')
    
    echo "${PHISHING_CANDIDATES}" > "${LOG_DIR}/phishing_analysis.json"
    
    # Check vulnerabilità note non patchate
    for system in $(cat "${LOG_DIR}/affected_systems.txt"); do
        nmap -sV --script vulners "${system}" > "${LOG_DIR}/vuln_scan_${system}.txt"
    done
    
    # Analisi log RDP/SSH per accessi anomali
    grep -E "(Failed|Accepted)" /var/log/auth.log | \
        awk '{print $1, $2, $3, $9, $11}' | \
        sort | uniq -c | sort -rn > "${LOG_DIR}/access_analysis.txt"
}

# STEP 4: Preservazione delle Evidenze
preserve_evidence() {
    log "STEP 4: Preservazione evidenze forensi"
    
    for system in $(cat "${LOG_DIR}/affected_systems.txt"); do
        # Dump memoria se accessibile
        if ping -c 1 ${system} &>/dev/null; then
            ssh forensics@${system} "sudo dd if=/dev/mem of=/tmp/mem.dump"
            scp forensics@${system}:/tmp/mem.dump "${LOG_DIR}/${system}_memory.dump"
        fi
        
        # Copia log critici
        rsync -avz forensics@${system}:/var/log/ "${LOG_DIR}/${system}_logs/"
        
        # Hash per chain of custody
        find "${LOG_DIR}/${system}_logs/" -type f -exec sha256sum {} \; \
            > "${LOG_DIR}/${system}_hashes.txt"
    done
}

# STEP 5: Comunicazione e Coordinamento
coordinate_response() {
    log "STEP 5: Coordinamento risposta"
    
    # Genera report preliminare
    cat > "${LOG_DIR}/preliminary_report.md" <<EOF
# Incident Report ${INCIDENT_ID}

## Executive Summary
- Tipo: Ransomware
- Sistemi affetti: $(wc -l < "${LOG_DIR}/affected_systems.txt")
- Impatto stimato: TBD
- Status: CONTENUTO

## Timeline
$(grep "STEP" "${LOG_DIR}/incident.log")

## Sistemi Affetti
$(cat "${LOG_DIR}/affected_systems.txt")

## Prossimi Passi
1. Analisi forense completa
2. Identificazione ransomware variant
3. Valutazione opzioni recovery
4. Comunicazione stakeholder
EOF
    
    # Notifica management
    mail -s "URGENT: Ransomware Incident ${INCIDENT_ID}" \
        ciso@gdo.com security-team@gdo.com < "${LOG_DIR}/preliminary_report.md"
    
    # Apertura ticket
    curl -X POST https://servicenow.internal/api/incident \
        -d "{
            \"priority\": 1,
            \"category\": \"security\",
            \"description\": \"Ransomware containment completed\",
            \"incident_id\": \"${INCIDENT_ID}\"
        }"
}

# Main execution
main() {
    mkdir -p "${LOG_DIR}"
    log "=== Iniziando risposta incidente Ransomware ==="
    
    isolate_affected_systems
    contain_lateral_movement
    identify_attack_vector
    preserve_evidence
    coordinate_response
    
    log "=== Contenimento completato. Procedere con analisi forense ==="
}

# Esecuzione con error handling
trap 'log "ERRORE: Runbook fallito al comando $BASH_COMMAND"' ERR
main "$@"
\end{lstlisting}

\section{D.4 Dashboard e KPI Templates}

\subsection{D.4.1 GIST Score Dashboard Configuration}
\begin{lstlisting}[language=json, caption=Configurazione Grafana per GIST Score Dashboard]
{
  "dashboard": {
    "title": "GIST Framework - Security Posture Dashboard",
    "panels": [
      {
        "title": "GIST Score Trend",
        "type": "graph",
        "targets": [
          {
            "expr": "gist_total_score",
            "legendFormat": "Total Score"
          },
          {
            "expr": "gist_component_physical",  
            "legendFormat": "Physical"
          },
          {
            "expr": "gist_component_architectural",
            "legendFormat": "Architectural"  
          },
          {
            "expr": "gist_component_security",
            "legendFormat": "Security"
          },
          {
            "expr": "gist_component_compliance",
            "legendFormat": "Compliance"
          }
        ]
      },
      {
        "title": "Attack Surface (ASSA)",
        "type": "gauge",
        "targets": [
          {
            "expr": "assa_score_current",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 500, "color": "yellow"},
                {"value": 800, "color": "orange"},
                {"value": 1000, "color": "red"}
              ]
            }
          }
        ]
      },
      {
        "title": "Compliance Status",
        "type": "stat",
        "targets": [
          {
            "expr": "compliance_score_pcidss",
            "title": "PCI-DSS"
          },
          {
            "expr": "compliance_score_gdpr",
            "title": "GDPR"
          },
          {
            "expr": "compliance_score_nis2",
            "title": "NIS2"
          }
        ]
      },
      {
        "title": "Security Incidents (24h)",
        "type": "table",
        "targets": [
          {
            "expr": "security_incidents_by_severity",
            "format": "table",
            "columns": ["time", "severity", "type", "affected_systems", "status"]
          }
        ]
      },
      {
        "title": "Infrastructure Health",
        "type": "heatmap",
        "targets": [
          {
            "expr": "infrastructure_health_by_location",
            "format": "heatmap"
          }
        ]
      }
    ],
    "refresh": "30s",
    "time": {
      "from": "now-24h",
      "to": "now"
    }
  }
}
\end{lstlisting}

%\end{document}