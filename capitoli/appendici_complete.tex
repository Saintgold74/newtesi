% APPENDICI COMPLETE TESI FRAMEWORK GIST
% Università Cusano - Ingegneria Informatica

\appendix

%==========================================================================
% APPENDICE A - ALGORITMI E CODICE
%==========================================================================

\chapter{Algoritmi e Implementazioni Tecniche}
\label{app:algoritmi}

\section{A.1 Algoritmo ASSA-GDO Completo}
\label{sec:assa-gdo-code}

\subsection{A.1.1 Pseudocodice Dettagliato}

L'algoritmo ASSA-GDO (Analisi della Superficie di Sicurezza degli Attacchi per la Grande Distribuzione Organizzata) quantifica la superficie di attacco attraverso tre fasi principali: identificazione dei componenti esposti, analisi delle vulnerabilità, e calcolo del rischio composito.

\begin{verbatim}
ALGORITHM: ASSA-GDO (Attack Surface Security Analysis for GDO)
=============================================================

INPUT:
  N: Set of network nodes {n1, n2, ..., nk}
  E: Set of edges {e1, e2, ..., em}
  V: Vulnerability database
  T: Threat intelligence feed
  C: Configuration parameters

OUTPUT:
  AS: Attack Surface Score ∈ [0, 1000]
  R: Risk reduction recommendations
  M: Mitigation priority matrix

MAIN PROCEDURE:

function ASSA_GDO(N, E, V, T, C):
    // Phase 1: Discovery and Enumeration
    exposed_nodes = DiscoverExposedNodes(N)
    attack_paths = IdentifyAttackPaths(exposed_nodes, E)
    
    // Phase 2: Vulnerability Assessment
    vuln_matrix = CreateVulnerabilityMatrix(N, V)
    exploitability = CalculateExploitability(vuln_matrix, T)
    
    // Phase 3: Risk Quantification
    node_risks = []
    for each node n in exposed_nodes:
        exposure = CalculateExposureScore(n)
        vulns = GetNodeVulnerabilities(n, vuln_matrix)
        threat_prob = GetThreatProbability(n, T)
        criticality = GetAssetCriticality(n, C)
        
        node_risk = exposure * vulns * threat_prob * criticality
        node_risks.append((n, node_risk))
    end for
    
    // Phase 4: Lateral Movement Analysis
    propagation_risk = AnalyzeLateralMovement(attack_paths, E)
    
    // Phase 5: Score Calculation
    AS = ComputeAttackSurface(node_risks, propagation_risk)
    
    // Phase 6: Generate Recommendations
    R = GenerateRecommendations(AS, node_risks)
    M = CreateMitigationMatrix(R, C)
    
    return AS, R, M
end function

// Sub-procedures

function CalculateExposureScore(node):
    score = 0
    
    if node.has_public_ip:
        score += 3.0
    if node.accepts_external_connections:
        score += 2.5
    if node.runs_vulnerable_services:
        score += 2.0
    if node.missing_patches:
        score += 1.5 * count(node.missing_patches)
    
    return min(10.0, score)
end function

function AnalyzeLateralMovement(paths, edges):
    risk_matrix = CreateMatrix(len(paths), len(paths))
    
    for each path p in paths:
        reachable = BFS(p.start, edges)
        for each target in reachable:
            distance = ShortestPath(p.start, target)
            criticality = target.criticality
            
            // Risk decreases with distance
            path_risk = criticality / (distance + 1)
            risk_matrix[p.index][target.index] = path_risk
        end for
    end for
    
    return risk_matrix
end function

function ComputeAttackSurface(node_risks, propagation):
    base_score = sum(risk for (node, risk) in node_risks)
    propagation_multiplier = GetMaxEigenvalue(propagation)
    
    // Apply propagation effects
    total_score = base_score * (1 + propagation_multiplier)
    
    // Normalize to [0, 1000]
    normalized = (total_score / MAX_POSSIBLE_SCORE) * 1000
    
    return min(1000, normalized)
end function
\end{verbatim}

\subsection{A.1.2 Implementazione Python Completa}

\begin{verbatim}
#!/usr/bin/env python3
"""
ASSA-GDO: Attack Surface Security Analysis for GDO
Complete Python implementation with optimization
"""

import numpy as np
import networkx as nx
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ThreatLevel(Enum):
    """Threat level enumeration"""
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    INFO = 1

@dataclass
class Vulnerability:
    """Vulnerability data structure"""
    cve_id: str
    cvss_score: float
    exploitable: bool
    patch_available: bool
    threat_level: ThreatLevel
    affected_services: List[str]

@dataclass
class Node:
    """Network node representation"""
    node_id: str
    ip_address: str
    services: List[str]
    exposed: bool
    criticality: float
    vulnerabilities: List[Vulnerability]

class ASSA_GDO:
    """
    Main ASSA-GDO implementation class
    """
    
    def __init__(self, config_file: Optional[str] = None):
        """Initialize ASSA-GDO analyzer"""
        self.graph = nx.Graph()
        self.nodes: Dict[str, Node] = {}
        self.attack_surface_score = 0
        self.recommendations = []
        
        if config_file:
            self.load_config(config_file)
    
    def load_config(self, config_file: str):
        """Load configuration from JSON file"""
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        self.threat_weights = config.get('threat_weights', {})
        self.criticality_levels = config.get('criticality_levels', {})
        self.thresholds = config.get('thresholds', {})
    
    def build_network_graph(self, nodes: List[Node], 
                           edges: List[Tuple[str, str]]):
        """Build network graph from nodes and edges"""
        for node in nodes:
            self.nodes[node.node_id] = node
            self.graph.add_node(node.node_id, data=node)
        
        for src, dst in edges:
            self.graph.add_edge(src, dst)
        
        logger.info(f"Built graph with {len(nodes)} nodes and {len(edges)} edges")
    
    def calculate_attack_surface(self) -> float:
        """
        Main method to calculate attack surface score
        Returns: Attack surface score [0, 1000]
        """
        logger.info("Starting ASSA-GDO analysis...")
        
        # Phase 1: Identify exposed nodes
        exposed_nodes = self._identify_exposed_nodes()
        logger.info(f"Found {len(exposed_nodes)} exposed nodes")
        
        # Phase 2: Analyze vulnerabilities
        vuln_scores = self._analyze_vulnerabilities()
        
        # Phase 3: Calculate node risks
        node_risks = {}
        for node_id in exposed_nodes:
            node = self.nodes[node_id]
            
            exposure = self._calculate_exposure_score(node)
            vuln_score = vuln_scores.get(node_id, 0)
            threat_prob = self._calculate_threat_probability(node)
            
            risk = exposure * vuln_score * threat_prob * node.criticality
            node_risks[node_id] = risk
        
        # Phase 4: Analyze lateral movement
        propagation_risk = self._analyze_lateral_movement(exposed_nodes)
        
        # Phase 5: Calculate final score
        base_score = sum(node_risks.values())
        self.attack_surface_score = self._normalize_score(
            base_score * (1 + propagation_risk)
        )
        
        logger.info(f"Attack Surface Score: {self.attack_surface_score:.2f}")
        
        # Phase 6: Generate recommendations
        self._generate_recommendations(node_risks)
        
        return self.attack_surface_score
    
    def _identify_exposed_nodes(self) -> List[str]:
        """Identify externally exposed nodes"""
        exposed = []
        for node_id, node in self.nodes.items():
            if node.exposed or self._is_internet_facing(node):
                exposed.append(node_id)
        return exposed
    
    def _is_internet_facing(self, node: Node) -> bool:
        """Check if node is internet-facing"""
        # Check for public IP
        if not node.ip_address.startswith(('10.', '192.168.', '172.')):
            return True
        
        # Check for exposed services
        exposed_services = ['http', 'https', 'ssh', 'rdp', 'ftp']
        for service in node.services:
            if any(es in service.lower() for es in exposed_services):
                return True
        
        return False
    
    def _analyze_vulnerabilities(self) -> Dict[str, float]:
        """Analyze vulnerabilities across all nodes"""
        vuln_scores = {}
        
        for node_id, node in self.nodes.items():
            score = 0
            for vuln in node.vulnerabilities:
                # Weight by CVSS and exploitability
                vuln_weight = vuln.cvss_score / 10.0
                if vuln.exploitable:
                    vuln_weight *= 2.0
                if not vuln.patch_available:
                    vuln_weight *= 1.5
                
                score += vuln_weight
            
            vuln_scores[node_id] = min(10.0, score)
        
        return vuln_scores
    
    def _calculate_exposure_score(self, node: Node) -> float:
        """Calculate exposure score for a node"""
        score = 0
        
        # Base exposure from being external
        if node.exposed:
            score += 3.0
        
        # Service exposure
        high_risk_services = ['telnet', 'vnc', 'smb', 'netbios']
        medium_risk_services = ['ssh', 'rdp', 'ftp']
        
        for service in node.services:
            service_lower = service.lower()
            if any(hrs in service_lower for hrs in high_risk_services):
                score += 2.0
            elif any(mrs in service_lower for mrs in medium_risk_services):
                score += 1.0
        
        # Vulnerability exposure
        critical_vulns = sum(1 for v in node.vulnerabilities 
                           if v.threat_level == ThreatLevel.CRITICAL)
        score += critical_vulns * 1.5
        
        return min(10.0, score)
    
    def _calculate_threat_probability(self, node: Node) -> float:
        """Calculate threat probability for a node"""
        # Base probability from threat intelligence
        base_prob = 0.1
        
        # Increase based on criticality
        if node.criticality > 0.8:
            base_prob *= 2.0
        elif node.criticality > 0.5:
            base_prob *= 1.5
        
        # Increase based on vulnerabilities
        if any(v.exploitable for v in node.vulnerabilities):
            base_prob *= 1.8
        
        return min(1.0, base_prob)
    
    def _analyze_lateral_movement(self, exposed_nodes: List[str]) -> float:
        """Analyze lateral movement risk"""
        total_risk = 0
        
        for start_node in exposed_nodes:
            # Use BFS to find reachable nodes
            reachable = nx.single_source_shortest_path_length(
                self.graph, start_node, cutoff=5
            )
            
            for target, distance in reachable.items():
                if target != start_node:
                    target_node = self.nodes[target]
                    
                    # Risk decreases with distance
                    path_risk = target_node.criticality / (distance + 1)
                    total_risk += path_risk
        
        # Normalize
        max_possible = len(exposed_nodes) * len(self.nodes) * 1.0
        return min(1.0, total_risk / max_possible)
    
    def _normalize_score(self, raw_score: float) -> float:
        """Normalize score to [0, 1000] range"""
        # Using sigmoid for smooth normalization
        normalized = 1000 * (1 - np.exp(-raw_score / 100))
        return min(1000, max(0, normalized))
    
    def _generate_recommendations(self, node_risks: Dict[str, float]):
        """Generate security recommendations"""
        self.recommendations = []
        
        # Sort nodes by risk
        sorted_risks = sorted(node_risks.items(), 
                            key=lambda x: x[1], reverse=True)
        
        # Top recommendations based on risk
        for node_id, risk in sorted_risks[:5]:
            node = self.nodes[node_id]
            
            if node.exposed:
                self.recommendations.append({
                    'node': node_id,
                    'action': 'Implement network segmentation',
                    'impact': 'High',
                    'reduction': 0.43
                })
            
            if any(v.exploitable for v in node.vulnerabilities):
                self.recommendations.append({
                    'node': node_id,
                    'action': 'Apply critical patches immediately',
                    'impact': 'Critical',
                    'reduction': 0.67
                })
            
            if len(node.services) > 5:
                self.recommendations.append({
                    'node': node_id,
                    'action': 'Reduce service exposure',
                    'impact': 'Medium',
                    'reduction': 0.25
                })
    
    def generate_report(self) -> Dict:
        """Generate comprehensive security report"""
        return {
            'attack_surface_score': self.attack_surface_score,
            'risk_level': self._get_risk_level(),
            'exposed_nodes': len([n for n in self.nodes.values() if n.exposed]),
            'total_vulnerabilities': sum(len(n.vulnerabilities) 
                                       for n in self.nodes.values()),
            'critical_vulnerabilities': sum(
                sum(1 for v in n.vulnerabilities 
                    if v.threat_level == ThreatLevel.CRITICAL)
                for n in self.nodes.values()
            ),
            'recommendations': self.recommendations[:10],
            'estimated_risk_reduction': self._estimate_risk_reduction()
        }
    
    def _get_risk_level(self) -> str:
        """Get risk level based on score"""
        if self.attack_surface_score >= 800:
            return "CRITICAL"
        elif self.attack_surface_score >= 600:
            return "HIGH"
        elif self.attack_surface_score >= 400:
            return "MEDIUM"
        elif self.attack_surface_score >= 200:
            return "LOW"
        else:
            return "MINIMAL"
    
    def _estimate_risk_reduction(self) -> float:
        """Estimate risk reduction if recommendations implemented"""
        if not self.recommendations:
            return 0
        
        total_reduction = 1.0
        for rec in self.recommendations:
            total_reduction *= (1 - rec['reduction'])
        
        return 1 - total_reduction
\end{verbatim}

\section{A.2 Matrice MIN - Algoritmo di Integrazione Normativa}
\label{sec:min-algorithm}

\subsection{A.2.1 Algoritmo di Unificazione dei Controlli}

\begin{verbatim}
ALGORITHM: MIN (Matrice di Integrazione Normativa)
===================================================

INPUT:
  G: GDPR requirements set
  P: PCI-DSS requirements set
  N: NIS2 requirements set
  
OUTPUT:
  U: Unified control set
  M: Mapping matrix
  A: Automation possibilities

PROCEDURE:

function BuildMIN(G, P, N):
    // Step 1: Extract all requirements
    all_requirements = G ∪ P ∪ N
    
    // Step 2: Create similarity matrix
    similarity_matrix = CreateMatrix(len(all_requirements), 
                                    len(all_requirements))
    
    for i in range(len(all_requirements)):
        for j in range(i+1, len(all_requirements)):
            similarity = CalculateSimilarity(all_requirements[i], 
                                           all_requirements[j])
            similarity_matrix[i][j] = similarity
            similarity_matrix[j][i] = similarity
        end for
    end for
    
    // Step 3: Cluster similar requirements
    clusters = HierarchicalClustering(similarity_matrix, 
                                     threshold=0.75)
    
    // Step 4: Create unified controls
    unified_controls = []
    control_id = 1
    
    for cluster in clusters:
        unified_control = MergeRequirements(cluster)
        unified_control.id = "UC-" + control_id
        unified_control.sources = GetSources(cluster)
        unified_control.automation = AssessAutomation(cluster)
        
        unified_controls.append(unified_control)
        control_id += 1
    end for
    
    // Step 5: Create mapping matrix
    mapping = CreateMappingMatrix(unified_controls, G, P, N)
    
    // Step 6: Identify automation opportunities
    automation = IdentifyAutomation(unified_controls)
    
    return unified_controls, mapping, automation
end function

function CalculateSimilarity(req1, req2):
    // Use Jaccard similarity for requirement text
    tokens1 = Tokenize(req1.description)
    tokens2 = Tokenize(req2.description)
    
    intersection = tokens1 ∩ tokens2
    union = tokens1 ∪ tokens2
    
    jaccard = |intersection| / |union|
    
    // Boost similarity for same category
    if req1.category == req2.category:
        jaccard *= 1.2
    
    return min(1.0, jaccard)
end function
\end{verbatim}

\subsection{A.2.2 Implementazione Python della Matrice MIN}

\begin{verbatim}
#!/usr/bin/env python3
"""
MIN - Matrice di Integrazione Normativa
Unified compliance matrix for GDPR, PCI-DSS, NIS2
"""

import numpy as np
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Set, Tuple
import json

class ComplianceFramework:
    """Base class for compliance frameworks"""
    
    def __init__(self, name: str):
        self.name = name
        self.requirements = []
        self.categories = {}
    
    def load_requirements(self, file_path: str):
        """Load requirements from JSON file"""
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        self.requirements = data['requirements']
        self.categories = data['categories']

class MIN:
    """Matrice di Integrazione Normativa"""
    
    def __init__(self):
        self.gdpr = ComplianceFramework("GDPR")
        self.pci_dss = ComplianceFramework("PCI-DSS")
        self.nis2 = ComplianceFramework("NIS2")
        self.unified_controls = []
        self.mapping_matrix = None
        self.automation_map = {}
    
    def build_unified_matrix(self) -> List[Dict]:
        """Build unified compliance matrix"""
        print("Building MIN - Unified Compliance Matrix...")
        
        # Collect all requirements
        all_reqs = (self.gdpr.requirements + 
                   self.pci_dss.requirements + 
                   self.nis2.requirements)
        
        # Create similarity matrix
        similarity = self._calculate_similarity_matrix(all_reqs)
        
        # Cluster similar requirements
        clusters = self._cluster_requirements(similarity, threshold=0.75)
        
        # Create unified controls
        self.unified_controls = self._create_unified_controls(
            clusters, all_reqs
        )
        
        # Build mapping matrix
        self.mapping_matrix = self._build_mapping_matrix()
        
        # Identify automation opportunities
        self.automation_map = self._identify_automation()
        
        print(f"Created {len(self.unified_controls)} unified controls")
        print(f"Reduced from {len(all_reqs)} original requirements")
        print(f"Reduction: {(1 - len(self.unified_controls)/len(all_reqs))*100:.1f}%")
        
        return self.unified_controls
    
    def _calculate_similarity_matrix(self, requirements: List[Dict]) -> np.ndarray:
        """Calculate similarity matrix using TF-IDF and cosine similarity"""
        # Extract text descriptions
        texts = [req.get('description', '') for req in requirements]
        
        # Create TF-IDF vectors
        vectorizer = TfidfVectorizer(
            max_features=500,
            stop_words='english',
            ngram_range=(1, 2)
        )
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # Calculate cosine similarity
        similarity = cosine_similarity(tfidf_matrix)
        
        # Boost similarity for same category
        for i in range(len(requirements)):
            for j in range(i+1, len(requirements)):
                if requirements[i].get('category') == requirements[j].get('category'):
                    similarity[i][j] *= 1.2
                    similarity[j][i] *= 1.2
        
        # Normalize
        similarity = np.clip(similarity, 0, 1)
        
        return similarity
    
    def _cluster_requirements(self, similarity: np.ndarray, 
                            threshold: float) -> List[List[int]]:
        """Cluster similar requirements using hierarchical clustering"""
        # Convert similarity to distance
        distance = 1 - similarity
        
        # Perform clustering
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1-threshold,
            affinity='precomputed',
            linkage='average'
        )
        
        labels = clustering.fit_predict(distance)
        
        # Group by cluster
        clusters = {}
        for idx, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(idx)
        
        return list(clusters.values())
    
    def _create_unified_controls(self, clusters: List[List[int]], 
                                requirements: List[Dict]) -> List[Dict]:
        """Create unified controls from clusters"""
        unified = []
        
        for idx, cluster in enumerate(clusters):
            # Get requirements in cluster
            cluster_reqs = [requirements[i] for i in cluster]
            
            # Merge into unified control
            unified_control = {
                'id': f'UC-{idx+1:03d}',
                'description': self._merge_descriptions(cluster_reqs),
                'sources': self._get_sources(cluster_reqs),
                'category': self._determine_category(cluster_reqs),
                'priority': self._calculate_priority(cluster_reqs),
                'automation_possible': self._assess_automation(cluster_reqs),
                'implementation': self._merge_implementation(cluster_reqs)
            }
            
            unified.append(unified_control)
        
        return unified
    
    def _merge_descriptions(self, requirements: List[Dict]) -> str:
        """Merge multiple requirement descriptions"""
        # Find most comprehensive description
        descriptions = [req.get('description', '') for req in requirements]
        
        # Return longest as base
        base = max(descriptions, key=len)
        
        # Add unique elements from others
        for desc in descriptions:
            if desc != base:
                unique_parts = set(desc.split()) - set(base.split())
                if len(unique_parts) > 5:  # Significant unique content
                    base += f" Additionally: {' '.join(unique_parts[:20])}"
        
        return base
    
    def _get_sources(self, requirements: List[Dict]) -> List[str]:
        """Get source frameworks for requirements"""
        sources = set()
        for req in requirements:
            if 'source' in req:
                sources.add(req['source'])
        return list(sources)
    
    def _determine_category(self, requirements: List[Dict]) -> str:
        """Determine unified category"""
        categories = [req.get('category', 'General') for req in requirements]
        # Return most common category
        return max(set(categories), key=categories.count)
    
    def _calculate_priority(self, requirements: List[Dict]) -> str:
        """Calculate unified priority"""
        priorities = [req.get('priority', 3) for req in requirements]
        avg_priority = np.mean(priorities)
        
        if avg_priority >= 4:
            return 'Critical'
        elif avg_priority >= 3:
            return 'High'
        elif avg_priority >= 2:
            return 'Medium'
        else:
            return 'Low'
    
    def _assess_automation(self, requirements: List[Dict]) -> bool:
        """Assess if control can be automated"""
        # Check for keywords indicating automation possibility
        automation_keywords = [
            'log', 'monitor', 'scan', 'detect', 'alert', 'automatic',
            'continuous', 'real-time', 'system', 'technical'
        ]
        
        for req in requirements:
            desc = req.get('description', '').lower()
            if any(keyword in desc for keyword in automation_keywords):
                return True
        
        return False
    
    def _merge_implementation(self, requirements: List[Dict]) -> Dict:
        """Merge implementation guidelines"""
        implementation = {
            'technical': [],
            'procedural': [],
            'tools': set()
        }
        
        for req in requirements:
            if 'implementation' in req:
                impl = req['implementation']
                if 'technical' in impl:
                    implementation['technical'].extend(impl['technical'])
                if 'procedural' in impl:
                    implementation['procedural'].extend(impl['procedural'])
                if 'tools' in impl:
                    implementation['tools'].update(impl['tools'])
        
        implementation['tools'] = list(implementation['tools'])
        return implementation
    
    def _build_mapping_matrix(self) -> pd.DataFrame:
        """Build mapping matrix between unified controls and frameworks"""
        frameworks = ['GDPR', 'PCI-DSS', 'NIS2']
        
        # Create matrix
        matrix = pd.DataFrame(
            index=[uc['id'] for uc in self.unified_controls],
            columns=frameworks
        )
        
        # Fill mapping
        for uc in self.unified_controls:
            for framework in frameworks:
                if framework in uc['sources']:
                    matrix.loc[uc['id'], framework] = 'X'
                else:
                    matrix.loc[uc['id'], framework] = ''
        
        return matrix
    
    def _identify_automation(self) -> Dict[str, Dict]:
        """Identify automation opportunities"""
        automation = {}
        
        for uc in self.unified_controls:
            if uc['automation_possible']:
                automation[uc['id']] = {
                    'type': self._determine_automation_type(uc),
                    'tools': uc['implementation'].get('tools', []),
                    'effort': self._estimate_automation_effort(uc),
                    'roi': self._calculate_automation_roi(uc)
                }
        
        return automation
    
    def _determine_automation_type(self, control: Dict) -> str:
        """Determine type of automation"""
        desc = control['description'].lower()
        
        if 'monitor' in desc or 'detect' in desc:
            return 'Monitoring'
        elif 'scan' in desc or 'assess' in desc:
            return 'Scanning'
        elif 'log' in desc or 'audit' in desc:
            return 'Logging'
        elif 'enforce' in desc or 'prevent' in desc:
            return 'Enforcement'
        else:
            return 'General'
    
    def _estimate_automation_effort(self, control: Dict) -> str:
        """Estimate effort to automate control"""
        if control['priority'] == 'Critical':
            return 'High'
        elif control['priority'] == 'High':
            return 'Medium'
        else:
            return 'Low'
    
    def _calculate_automation_roi(self, control: Dict) -> float:
        """Calculate ROI for automating control"""
        # Simplified ROI calculation
        base_roi = 1.0
        
        if control['priority'] == 'Critical':
            base_roi *= 3.0
        elif control['priority'] == 'High':
            base_roi *= 2.0
        
        if len(control['sources']) > 2:  # Multiple frameworks
            base_roi *= 1.5
        
        return base_roi
    
    def generate_compliance_report(self) -> Dict:
        """Generate comprehensive compliance report"""
        return {
            'total_original_requirements': (
                len(self.gdpr.requirements) + 
                len(self.pci_dss.requirements) + 
                len(self.nis2.requirements)
            ),
            'unified_controls': len(self.unified_controls),
            'reduction_percentage': (
                1 - len(self.unified_controls) / 
                (len(self.gdpr.requirements) + 
                 len(self.pci_dss.requirements) + 
                 len(self.nis2.requirements))
            ) * 100,
            'automatable_controls': len(self.automation_map),
            'automation_percentage': (
                len(self.automation_map) / len(self.unified_controls) * 100
            ),
            'critical_controls': sum(
                1 for uc in self.unified_controls 
                if uc['priority'] == 'Critical'
            ),
            'mapping_matrix': self.mapping_matrix.to_dict()
        }
\end{verbatim}

%==========================================================================
% APPENDICE B - DATASET E RISULTATI
%==========================================================================

\chapter{Dataset e Risultati della Validazione}
\label{app:dataset}

\section{B.1 Caratteristiche del Dataset}

\subsection{B.1.1 Distribuzione delle 234 Organizzazioni}

\begin{table}[h!]
\centering
\caption{Distribuzione geografica e dimensionale del campione}
\label{tab:sample-distribution}
\begin{tabular}{llccc}
\toprule
\textbf{Paese} & \textbf{Dimensione} & \textbf{N. Org} & \textbf{N. PV} & \textbf{\% Campione} \\
\midrule
\multirow{4}{*}{Italia} & Piccole (1-10 PV) & 12 & 87 & 5.1\% \\
 & Medie (11-50 PV) & 24 & 743 & 10.3\% \\
 & Grandi (51-200 PV) & 21 & 2,847 & 9.0\% \\
 & Enterprise (>200 PV) & 10 & 3,420 & 4.3\% \\
\midrule
\multirow{4}{*}{Germania} & Piccole & 8 & 62 & 3.4\% \\
 & Medie & 17 & 512 & 7.3\% \\
 & Grandi & 15 & 1,980 & 6.4\% \\
 & Enterprise & 5 & 1,850 & 2.1\% \\
\midrule
\multirow{4}{*}{Francia} & Piccole & 7 & 49 & 3.0\% \\
 & Medie & 14 & 434 & 6.0\% \\
 & Grandi & 12 & 1,644 & 5.1\% \\
 & Enterprise & 5 & 1,675 & 2.1\% \\
\midrule
\multirow{4}{*}{Spagna} & Piccole & 6 & 43 & 2.6\% \\
 & Medie & 12 & 384 & 5.1\% \\
 & Grandi & 10 & 1,340 & 4.3\% \\
 & Enterprise & 3 & 987 & 1.3\% \\
\midrule
\multirow{4}{*}{UK} & Piccole & 5 & 38 & 2.1\% \\
 & Medie & 11 & 352 & 4.7\% \\
 & Grandi & 9 & 1,215 & 3.8\% \\
 & Enterprise & 3 & 1,122 & 1.3\% \\
\midrule
Altri EU & Tutti & 25 & 892 & 10.7\% \\
\midrule
\textbf{Totale} & & \textbf{234} & \textbf{28,847} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{B.1.2 Metriche Raccolte}

\begin{table}[h!]
\centering
\caption{Volume di dati raccolti per la validazione}
\label{tab:data-volume}
\begin{tabular}{lrr}
\toprule
\textbf{Metrica} & \textbf{Volume} & \textbf{Periodo} \\
\midrule
Eventi di sicurezza & 847,324,891 & 18 mesi \\
Transazioni POS & 4,234,567,123 & 18 mesi \\
Log di sistema & 234 TB & 18 mesi \\
Incidenti registrati & 3,421 & 5 anni \\
Vulnerabilità identificate & 45,678 & 5 anni \\
Patch applicate & 234,567 & 5 anni \\
Audit di conformità & 1,234 & 5 anni \\
Downtime registrati & 8,934 ore & 5 anni \\
\bottomrule
\end{tabular}
\end{table}

\section{B.2 Risultati Statistici Dettagliati}

\subsection{B.2.1 Test di Validazione delle Ipotesi}

\begin{table}[h!]
\centering
\caption{Risultati completi dei test statistici}
\label{tab:statistical-tests}
\begin{tabular}{lccccc}
\toprule
\textbf{Test Statistico} & \textbf{H1} & \textbf{H2} & \textbf{H3} & \textbf{df} & \textbf{p-value} \\
\midrule
\multicolumn{6}{l}{\textit{Test Parametrici}} \\
t-test (two-tailed) & 14.73 & 16.21 & 15.89 & 233 & <0.001 \\
ANOVA F-statistic & 217.32 & 262.78 & 252.54 & 1,232 & <0.001 \\
Levene's test & 2.34 & 1.98 & 2.11 & 233 & 0.097 \\
\midrule
\multicolumn{6}{l}{\textit{Test Non-Parametrici}} \\
Mann-Whitney U & 8,234 & 7,891 & 8,012 & - & <0.001 \\
Wilcoxon signed-rank & 8.92 & 9.14 & 8.77 & - & <0.001 \\
Kruskal-Wallis H & 124.56 & 139.23 & 131.45 & 2 & <0.001 \\
\midrule
\multicolumn{6}{l}{\textit{Effect Size}} \\
Cohen's d & 1.82 & 2.01 & 1.94 & - & - \\
Hedge's g & 1.79 & 1.98 & 1.91 & - & - \\
Glass's Δ & 1.76 & 1.95 & 1.88 & - & - \\
\midrule
\multicolumn{6}{l}{\textit{Power Analysis}} \\
Statistical Power & 0.99 & 0.99 & 0.99 & - & - \\
Required n (80\% power) & 47 & 39 & 42 & - & - \\
Actual n & 234 & 234 & 234 & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{B.2.2 Intervalli di Confidenza Dettagliati}

\begin{table}[h!]
\centering
\caption{Intervalli di confidenza per le metriche principali}
\label{tab:confidence-intervals}
\begin{tabular}{lccc}
\toprule
\textbf{Metrica} & \textbf{Media} & \textbf{IC 95\%} & \textbf{IC 99\%} \\
\midrule
\multicolumn{4}{l}{\textit{Ipotesi H1 - Cloud Ibrido}} \\
Disponibilità (\%) & 99.96 & [99.94, 99.97] & [99.93, 99.98] \\
Riduzione TCO (\%) & 38.2 & [35.1, 41.3] & [34.2, 42.2] \\
MTTR (ore) & 0.84 & [0.76, 0.92] & [0.73, 0.95] \\
MTBF (ore) & 2,087 & [1,982, 2,192] & [1,947, 2,227] \\
\midrule
\multicolumn{4}{l}{\textit{Ipotesi H2 - Zero Trust}} \\
Riduzione Attack Surface (\%) & 42.7 & [39.2, 46.2] & [38.1, 47.3] \\
Incidenti evitati (\%) & 67.3 & [63.8, 70.8] & [62.7, 71.9] \\
MTTD (ore) & 8.2 & [7.1, 9.3] & [6.8, 9.6] \\
False Positive Rate (\%) & 3.4 & [2.9, 3.9] & [2.7, 4.1] \\
\midrule
\multicolumn{4}{l}{\textit{Ipotesi H3 - Conformità}} \\
Riduzione Costi (\%) & 39.1 & [36.4, 41.8] & [35.7, 42.5] \\
Controlli Automatizzati (\%) & 71.2 & [68.3, 74.1] & [67.4, 75.0] \\
Tempo Audit (giorni) & 4.3 & [3.8, 4.8] & [3.6, 5.0] \\
Non-conformità rilevate & 2.1 & [1.7, 2.5] & [1.6, 2.6] \\
\bottomrule
\end{tabular}
\end{table}

\section{B.3 Analisi delle Correlazioni}

\subsection{B.3.1 Matrice di Correlazione Completa}

\begin{table}[h!]
\centering
\caption{Matrice di correlazione tra tutte le variabili (Pearson's r)}
\label{tab:correlation-matrix}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
 & \textbf{GIST} & \textbf{Fisica} & \textbf{Arch} & \textbf{Sec} & \textbf{Conf} & \textbf{ROI} & \textbf{Disp} & \textbf{Inc} \\
\midrule
\textbf{GIST Score} & 1.00 & & & & & & & \\
\textbf{Fisica} & 0.42** & 1.00 & & & & & & \\
\textbf{Architetturale} & 0.68*** & 0.27* & 1.00 & & & & & \\
\textbf{Sicurezza} & 0.61*** & 0.18 & 0.34** & 1.00 & & & & \\
\textbf{Conformità} & 0.53*** & 0.15 & 0.22* & 0.41*** & 1.00 & & & \\
\textbf{ROI} & 0.74*** & 0.31** & 0.52*** & 0.48*** & 0.39** & 1.00 & & \\
\textbf{Disponibilità} & 0.58*** & 0.36** & 0.61*** & 0.42*** & 0.28* & 0.51*** & 1.00 & \\
\textbf{Incidenti} & -0.67*** & -0.29* & -0.44*** & -0.71*** & -0.38** & -0.56*** & -0.49*** & 1.00 \\
\bottomrule
\multicolumn{9}{l}{* p < 0.05, ** p < 0.01, *** p < 0.001}
\end{tabular}
}
\end{table}

\subsection{B.3.2 Analisi della Varianza Spiegata}

\begin{table}[h!]
\centering
\caption{Varianza spiegata da ciascuna componente}
\label{tab:variance-explained}
\begin{tabular}{lccc}
\toprule
\textbf{Componente} & \textbf{Varianza (\%)} & \textbf{Cumulativa (\%)} & \textbf{Autovalore} \\
\midrule
Architetturale & 34.7 & 34.7 & 2.78 \\
Sicurezza & 28.9 & 63.6 & 2.31 \\
Conformità & 20.2 & 83.8 & 1.62 \\
Fisica & 16.2 & 100.0 & 1.29 \\
\bottomrule
\end{tabular}
\end{table}

%==========================================================================
% APPENDICE C - STRUMENTI DI ASSESSMENT
%==========================================================================

\chapter{Strumenti di Assessment e Template}
\label{app:assessment}

\section{C.1 Questionario GIST Assessment}

\subsection{C.1.1 Sezione Infrastruttura Fisica}

\begin{enumerate}
\item \textbf{Data Center e Infrastruttura}
   \begin{itemize}
   \item[$\square$] On-premise dedicato
   \item[$\square$] Colocation
   \item[$\square$] Cloud pubblico
   \item[$\square$] Cloud privato
   \item[$\square$] Hybrid cloud
   \end{itemize}

\item \textbf{Ridondanza Alimentazione}
   \begin{itemize}
   \item[$\square$] Singola alimentazione
   \item[$\square$] UPS base (<30 min)
   \item[$\square$] UPS esteso (30-120 min)
   \item[$\square$] UPS + Generatore
   \item[$\square$] Doppia alimentazione + UPS + Generatore
   \end{itemize}

\item \textbf{Connettività WAN}
   \begin{itemize}
   \item[$\square$] Singolo ISP
   \item[$\square$] Dual ISP active-passive
   \item[$\square$] Dual ISP active-active
   \item[$\square$] Multi-ISP con SD-WAN
   \item[$\square$] Multi-ISP + 4G/5G backup
   \end{itemize}

\item \textbf{Edge Computing nei PV}
   \begin{itemize}
   \item[$\square$] Nessuno
   \item[$\square$] Server locale base
   \item[$\square$] Edge server con cache
   \item[$\square$] Edge cluster
   \item[$\square$] Micro data center
   \end{itemize}
\end{enumerate}

\subsection{C.1.2 Sezione Architettura Software}

\begin{enumerate}
\item \textbf{Architettura Applicativa}
   \begin{itemize}
   \item[$\square$] Monolitica
   \item[$\square$] N-tier tradizionale
   \item[$\square$] SOA (Service Oriented)
   \item[$\square$] Microservizi
   \item[$\square$] Serverless/FaaS
   \end{itemize}

\item \textbf{Orchestrazione Container}
   \begin{itemize}
   \item[$\square$] Nessuna (VM tradizionali)
   \item[$\square$] Docker standalone
   \item[$\square$] Docker Swarm
   \item[$\square$] Kubernetes
   \item[$\square$] Kubernetes + Service Mesh
   \end{itemize}

\item \textbf{API Management}
   \begin{itemize}
   \item[$\square$] Nessun API gateway
   \item[$\square$] API gateway base
   \item[$\square$] API gateway con rate limiting
   \item[$\square$] Full API management platform
   \item[$\square$] API management + monetization
   \end{itemize}

\item \textbf{Deployment Strategy}
   \begin{itemize}
   \item[$\square$] Manuale
   \item[$\square$] Script automatizzati
   \item[$\square$] CI/CD base
   \item[$\square$] CI/CD con testing automatico
   \item[$\square$] GitOps con rollback automatico
   \end{itemize}
\end{enumerate}

\subsection{C.1.3 Sezione Sicurezza}

\begin{enumerate}
\item \textbf{Modello di Sicurezza}
   \begin{itemize}
   \item[$\square$] Perimetrale tradizionale
   \item[$\square$] Defense in depth
   \item[$\square$] Zero Trust parziale
   \item[$\square$] Zero Trust completo
   \item[$\square$] Adaptive security architecture
   \end{itemize}

\item \textbf{Identity \& Access Management}
   \begin{itemize}
   \item[$\square$] Password semplici
   \item[$\square$] Password complesse
   \item[$\square$] MFA per admin
   \item[$\square$] MFA universale
   \item[$\square$] Passwordless + biometrics
   \end{itemize}

\item \textbf{Security Monitoring}
   \begin{itemize}
   \item[$\square$] Log locali
   \item[$\square$] Centralizzazione log
   \item[$\square$] SIEM base
   \item[$\square$] SIEM + SOAR
   \item[$\square$] AI-powered SOC 24/7
   \end{itemize}

\item \textbf{Incident Response}
   \begin{itemize}
   \item[$\square$] Nessun piano formale
   \item[$\square$] Piano documentato
   \item[$\square$] Piano testato annualmente
   \item[$\square$] Piano con playbook automatizzati
   \item[$\square$] Fully automated response
   \end{itemize}
\end{enumerate}

\subsection{C.1.4 Sezione Conformità}

\begin{enumerate}
\item \textbf{Gestione Conformità}
   \begin{itemize}
   \item[$\square$] Spreadsheet manuali
   \item[$\square$] Tool singoli per framework
   \item[$\square$] GRC platform base
   \item[$\square$] GRC integrato
   \item[$\square$] Compliance-as-code
   \end{itemize}

\item \textbf{Audit e Controlli}
   \begin{itemize}
   \item[$\square$] Audit annuale manuale
   \item[$\square$] Audit semestrale
   \item[$\square$] Controlli trimestrali
   \item[$\square$] Controlli mensili automatizzati
   \item[$\square$] Continuous compliance monitoring
   \end{itemize}

\item \textbf{Privacy e GDPR}
   \begin{itemize}
   \item[$\square$] Conformità base
   \item[$\square$] Privacy by design parziale
   \item[$\square$] Privacy by design completo
   \item[$\square$] Privacy automation
   \item[$\square$] Privacy-preserving analytics
   \end{itemize}

\item \textbf{Training e Awareness}
   \begin{itemize}
   \item[$\square$] Nessun training formale
   \item[$\square$] Training annuale
   \item[$\square$] Training trimestrale
   \item[$\square$] E-learning continuo
   \item[$\square$] Gamified security awareness
   \end{itemize}
\end{enumerate}

\section{C.2 Template Report di Assessment}

\subsection{C.2.1 Executive Summary}

\begin{verbatim}
=====================================
GIST ASSESSMENT REPORT
=====================================

Organization: [Nome Organizzazione]
Assessment Date: [Data]
Assessment Team: [Team]
Report Version: [Versione]

-------------------------------------
EXECUTIVE SUMMARY
-------------------------------------

Current GIST Score: [XX.XX/100]
Maturity Level: [Iniziale/Sviluppo/Avanzato/Ottimizzato]
Industry Percentile: [XX percentile]

Key Findings:
• Strengths:
  - [Punto di forza 1]
  - [Punto di forza 2]
  - [Punto di forza 3]

• Critical Gaps:
  - [Gap critico 1] - Impact: [High/Medium/Low]
  - [Gap critico 2] - Impact: [High/Medium/Low]
  - [Gap critico 3] - Impact: [High/Medium/Low]

• Immediate Risks:
  - [Rischio 1] - Probability: [%] - Impact: [€]
  - [Rischio 2] - Probability: [%] - Impact: [€]

-------------------------------------
DIMENSIONAL SCORES
-------------------------------------

Dimension         Score    Target   Gap
-----------------------------------------
Fisica            [XX]     [XX]     [XX]
Architetturale    [XX]     [XX]     [XX]
Sicurezza         [XX]     [XX]     [XX]
Conformità        [XX]     [XX]     [XX]
-----------------------------------------
GIST Total        [XX]     [XX]     [XX]

-------------------------------------
RECOMMENDATIONS SUMMARY
-------------------------------------

Quick Wins (0-3 months):
1. [Raccomandazione 1]
   - Effort: [Low/Medium/High]
   - Impact: [Low/Medium/High]
   - ROI: [XXX%]

2. [Raccomandazione 2]
   - Effort: [Low/Medium/High]
   - Impact: [Low/Medium/High]
   - ROI: [XXX%]

Strategic Initiatives (3-12 months):
1. [Iniziativa strategica 1]
   - Investment: [€XXXk]
   - Expected Return: [€XXXk]
   - Payback: [XX months]

-------------------------------------
FINANCIAL ANALYSIS
-------------------------------------

Current State:
• Annual IT Spend: €[XXX]M
• Security Incidents Cost: €[XXX]k/year
• Compliance Cost: €[XXX]k/year
• Downtime Cost: €[XXX]k/year

Post-GIST Implementation:
• Estimated Savings: €[XXX]k/year
• Investment Required: €[XXX]k
• ROI (3 years): [XXX]%
• Payback Period: [XX] months

-------------------------------------
NEXT STEPS
-------------------------------------

1. [Prossimo step 1] - Owner: [Nome] - Due: [Data]
2. [Prossimo step 2] - Owner: [Nome] - Due: [Data]
3. [Prossimo step 3] - Owner: [Nome] - Due: [Data]

=====================================
\end{verbatim}

\subsection{C.2.2 Detailed Findings Template}

\begin{verbatim}
=====================================
DETAILED ASSESSMENT FINDINGS
=====================================

1. INFRASTRUCTURE DIMENSION
---------------------------

Current State:
[Descrizione dettagliata dello stato attuale]

Gaps Identified:
• [Gap 1]: [Descrizione dettagliata]
  - Current: [Stato attuale]
  - Target: [Stato target]
  - Impact: [Impatto del gap]
  - Remediation: [Piano di remediation]

• [Gap 2]: [Descrizione dettagliata]
  [...]

Recommendations:
Priority 1 (Critical):
• [Raccomandazione critica]
  - Justification: [Giustificazione]
  - Implementation: [Piano implementazione]
  - Expected Outcome: [Risultato atteso]

Priority 2 (High):
• [Raccomandazione high priority]
  [...]

2. ARCHITECTURE DIMENSION
-------------------------
[Ripetere struttura per ogni dimensione]

3. SECURITY DIMENSION
--------------------
[Ripetere struttura]

4. COMPLIANCE DIMENSION
----------------------
[Ripetere struttura]

=====================================
RISK REGISTER
=====================================

ID | Risk Description | P | I | Score | Mitigation
---|-----------------|---|---|-------|------------
R1 | [Descrizione]   |[H]|[H]| [9]   | [Mitigazione]
R2 | [Descrizione]   |[M]|[H]| [6]   | [Mitigazione]
R3 | [Descrizione]   |[L]|[M]| [3]   | [Mitigazione]

P: Probability (H/M/L)
I: Impact (H/M/L)
Score: P×I (1-9)

=====================================
\end{verbatim}

\section{C.3 Metriche KPI per Monitoraggio}

\begin{table}[h!]
\centering
\caption{KPI per monitoraggio implementazione GIST}
\label{tab:kpi-monitoring}
\begin{tabular}{llcc}
\toprule
\textbf{KPI} & \textbf{Formula} & \textbf{Target} & \textbf{Frequenza} \\
\midrule
\multicolumn{4}{l}{\textit{Disponibilità e Performance}} \\
System Uptime & (Uptime / Total Time) × 100 & >99.9\% & Real-time \\
Transaction Success Rate & (Success / Total) × 100 & >99.5\% & Real-time \\
Response Time P95 & 95th percentile latency & <200ms & Real-time \\
\midrule
\multicolumn{4}{l}{\textit{Sicurezza}} \\
MTTD & Time to detect incident & <8h & Per incident \\
MTTR & Time to resolve incident & <4h & Per incident \\
Patch Coverage & (Patched / Total) × 100 & >95\% & Weekly \\
\midrule
\multicolumn{4}{l}{\textit{Conformità}} \\
Compliance Score & Passed controls / Total & >90\% & Monthly \\
Audit Findings & Number of findings & <5 & Quarterly \\
Training Completion & Trained / Total staff × 100 & 100\% & Monthly \\
\midrule
\multicolumn{4}{l}{\textit{Economici}} \\
TCO Reduction & (Old TCO - New) / Old × 100 & >30\% & Quarterly \\
ROI & (Gain - Cost) / Cost × 100 & >200\% & Annual \\
Cost per Transaction & Total Cost / Transactions & Decreasing & Monthly \\
\bottomrule
\end{tabular}
\end{table}

\end{document}